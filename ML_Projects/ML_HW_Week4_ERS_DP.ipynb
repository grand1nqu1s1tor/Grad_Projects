{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3IG7eLX25X3"
      },
      "source": [
        "# Programming Assignment 4 - Simple Linear vs. Ridge Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv_oZqQI25X4"
      },
      "source": [
        "In the historical heart of Boston, Bob seeks to understand the intricacies of the real estate market. With a linear regression model at his side, Bob wonders if he can improve his predictions. Given your expertise in machine learning, he turns to you for guidance. Specifically, he wants to unravel the factors influencing the median value of homes across different Boston neighborhoods.\n",
        "\n",
        "To assist Bob, you decide to:\n",
        "*  Implement the closed-form solution for linear regression.\n",
        "* Apply a polynomial transformation to increase model flexibility.\n",
        "* Utilize ridge regression to control model complexity.\n",
        "* Apply 10-fold cross-validation for more reliable performance estimates.\n",
        "\n",
        "\n",
        "Bob is curious and wants to see a comparison between linear and ridge regression, both with and without polynomial transformations, on the same dataset. Thus, the challenge begins!\n",
        "\n",
        " Variables in order:\n",
        "* CRIM:     per capita crime rate by town\n",
        "*  ZN:       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "* INDUS:    proportion of non-retail business acres per town\n",
        "* CHAS:     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "* NOX:      nitric oxides concentration (parts per 10 million)\n",
        "* RM:       average number of rooms per dwelling\n",
        "* AGE:      proportion of owner-occupied units built prior to 1940\n",
        "* DIS:      weighted distances to five Boston employment centres\n",
        "* RAD:      index of accessibility to radial highways\n",
        "* TAX:      full-value property-tax rate per \\$10,000\n",
        "* PTRATIO:  pupil-teacher ratio by town\n",
        "* B:        $1000(Bk - 0.63)^2$ where Bk is the proportion of blacks by town\n",
        "* LSTAT:    \\% lower status of the population\n",
        "* MEDV:     Median value of owner-occupied homes in \\$1000's\n",
        "\n",
        "Note: The Boston Housing dataset, especially the 'B' variable, touches upon serious ethical and societal concerns related to race and inequality. Reflect upon these issues, and consider strategies such as excluding the 'B' column from analyses.\n",
        "\n",
        "With this context, let's assist Bob in his real estate endeavors!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_hCd1yi25X5"
      },
      "source": [
        "## 1 Setup and Data Preparation\n",
        "Import Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "eZ3eAVSK25X5"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Fundamental package for linear algebra and multidimensional arrays\n",
        "import pandas as pd  # Data analysis and manipulation tool\n",
        "\n",
        "# Transform features to polynomial features for model flexibility\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Split arrays or matrices into random train and test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Scale features to zero mean and unit variance, commonly used for normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Provides train/test indices to split data into train/test sets while performing cross-validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Calculates MSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDi0L0Fg25X5"
      },
      "source": [
        "Load the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature names\n",
        "# Specifying the names of the columns in our dataset makes it easier to understand and reference them.\n",
        "feature_names = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"RAD\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n",
        "\n",
        "# Load the data\n",
        "# We read data from a CSV (Comma-Separated Values) file into a DataFrame. DataFrame is a 2D labeled data structure in pandas.\n",
        "filename = '/content/Boston_housing.csv'\n",
        "df = pd.read_csv(filename, sep='\\s+', header=None)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "# It's good practice to inspect the dataset's size and first few rows to ensure it's loaded correctly and understand its structure.\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Extract features and target\n",
        "# Machine learning typically involves using features (independent variables) to predict a target (dependent variable).\n",
        "# Here, we separate the dataset into features (X) and target (y).\n",
        "X = np.array(df.iloc[:, :13])  # All columns up to the 13th are features\n",
        "y = np.array(df.iloc[:, 13]).reshape(-1, 1)  # The 13th column is our target, and we reshape it to a 2D array for compatibility.\n",
        "\n",
        "# Preview data\n",
        "# It's also good practice to preview the data after separation to ensure everything looks as expected.\n",
        "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n",
        "print(\"First 5 values of y:\\n\", y[:5])\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyoPc-rWPD2D",
        "outputId": "964e01e8-b8e7-441e-ef11-f6d220b15077"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (506, 14)\n",
            "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
            "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
            "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
            "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
            "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
            "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
            "\n",
            "       11    12    13  \n",
            "0  396.90  4.98  24.0  \n",
            "1  396.90  9.14  21.6  \n",
            "2  392.83  4.03  34.7  \n",
            "3  394.63  2.94  33.4  \n",
            "4  396.90  5.33  36.2  \n",
            "\n",
            "First 5 rows of X:\n",
            " [[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
            "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
            "  4.9800e+00]\n",
            " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
            "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
            "  9.1400e+00]\n",
            " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
            "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
            "  4.0300e+00]\n",
            " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
            "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
            "  2.9400e+00]\n",
            " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
            "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
            "  5.3300e+00]]\n",
            "First 5 values of y:\n",
            " [[24. ]\n",
            " [21.6]\n",
            " [34.7]\n",
            " [33.4]\n",
            " [36.2]]\n",
            "X shape: (506, 13)\n",
            "y shape: (506, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQtVbr0t25X6"
      },
      "source": [
        "Checking for missing values\n",
        "\n",
        "After getting the data, it's always a good practice to check for missing values in the dataset. Luckily for us, this dataset has no missing values. Here's how you can verify that:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcDVDVdF25X6",
        "outputId": "42af1741-77ed-4152-8448-63fa17a5f180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in X: 0\n",
            "Missing values in y: 0\n"
          ]
        }
      ],
      "source": [
        "# 2. Check for Missing Values:\n",
        "print(\"Missing values in X:\", np.isnan(X).sum())\n",
        "print(\"Missing values in y:\", np.isnan(y).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOD0lvRO25X6"
      },
      "source": [
        "## Implementing 10-Fold Cross-Validation\n",
        "With the data now loaded into X and y, your next task is to implement the code to select the optimal regularization and polynomial transformation. Utilize 10-fold cross-validation to assess the various configurations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8OKZ48E25X6"
      },
      "source": [
        "## 10-Fold Cross-Validation with Feature Scaling and Polynomial Transformation\n",
        "\n",
        "Cross-validation is a method to assess the performance of a machine learning model on unseen data by dividing the data into a set number of groups, or \"folds\".\n",
        "\n",
        "### Why 10-Fold Cross-Validation?\n",
        "\n",
        "In 10-fold cross-validation, the dataset is randomly divided into ten parts or folds. The idea is to iteratively train the model on 9 of these folds and test it on the tenth. This is done ten times, once for each fold acting as the validation set. By doing so, we're ensuring that each data point gets to be in a validation set exactly once.\n",
        "\n",
        "### Feature Scaling Within Cross-Validation\n",
        "\n",
        "Feature scaling ensures that all features contribute equally to the model performance, which is particularly important for algorithms sensitive to feature magnitudes.\n",
        "\n",
        "When doing cross-validation, it's crucial that we don't introduce data leakage by scaling using statistics from the entire dataset. Instead:\n",
        "1. Divide the data into training and validation sets.\n",
        "2. Fit the scaler on the training set.\n",
        "3. Apply the scaling to both the training and validation sets using this scaler.\n",
        "\n",
        "### Polynomial Transformation Within Cross-Validation\n",
        "\n",
        "Polynomial transformations capture more intricate data relationships by adding polynomial features. Here's how you incorporate it into cross-validation:\n",
        "1. Divide the data into training and validation sets.\n",
        "2. Fit the polynomial transformer on the training set.\n",
        "3. Transform both the training and validation sets using this transformer.\n",
        "4. Fit the scaler on the transformed training set\n",
        "4. Apply the scaling to both the transformed training and transformed validation sets using this scaler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odSu-RCY25X7"
      },
      "source": [
        "---\n",
        "### Note on Cross-Validation Error Calculation\n",
        "\n",
        "In most lecture notes and literature on k-fold cross-validation, the procedure for calculating the cross-validation error typically involves computing the mean of the errors obtained from each fold. However, in the context of our analysis, given the relatively small size of the dataset and the possibility of unequal numbers of samples in each fold, this traditional approach might not be mathematically rigorous.\n",
        "\n",
        "To address this, our approach for calculating the cross-validation error will deviate slightly from the traditional method. Instead of merely averaging the errors from each fold, we will sum up the errors across all folds and then divide by $ N $, the total number of training examples. This ensures that our error estimate is unbiased and takes into account the potential discrepancy in the number of samples across different folds.\n",
        "\n",
        "Mathematically, the cross-validation error, $ E_{cv} $, for this assignment is computed as:\n",
        "$$  E_{\\text{cv}} = \\frac{1}{N} \\sum_{i=1}^{k} \\sum_{j \\in \\text{fold } i} (y^{(j)}- \\hat{y}^{(j)})^2\n",
        " $$\n",
        "where $ k $ is the number of folds, $ y^{(j)} $ is the true target value of the $j^{th} $ example, and $ \\hat{y}^{(j)} $ is the predicted value for the same example.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HztqrdTA25X7"
      },
      "source": [
        "# Your code goes here\n",
        "\n",
        "Feel free to add any helper functions you may need.\n",
        "\n",
        "### Part a) 10-fold Cross Validation using Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "1oqSlins25X7"
      },
      "outputs": [],
      "source": [
        "def linear_regression(X, y):\n",
        "    # use np.linalg.pinv(a)\n",
        "    # Compute the weights using the closed-form solution\n",
        "    #### TO-DO #####\n",
        "    return np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "    ##############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0DRiolI25X7"
      },
      "source": [
        " Next implement Squared Error. It measures the average squared difference between the estimated values (predictions) and the actual values (true values). Mathematically, it is represented as: $  \\sum_{i=1}^{N} (y^{(i)} - \\hat{y}^{(i)})^2 $\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "OjNWAbfC25X7"
      },
      "outputs": [],
      "source": [
        "def squared_error(y_test, y_pred):\n",
        "    #### TO-DO #####\n",
        "    # Calculate the squared differences\n",
        "    return mean_squared_error(y_test, y_pred)\n",
        "    ##############\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dF99sRC25X7",
        "outputId": "05a01440-5e3b-45b2-dd0a-deca66213921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-Fold Linear Regression\n",
            "e_in:21.818586996144017\n",
            "e_cv:23.364203007530946\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21.818586996144017, 23.364203007530946)"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def k_fold_linear_regression(X, y, k=10):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross-validation for linear regression.\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=k, random_state=42, shuffle=True)\n",
        "\n",
        "    cve_values = []\n",
        "    ise_values = []\n",
        "\n",
        "    for train_index, val_index in kf.split(X):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # Scaling the features without data leakage\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "        # Adding a bias term (constant column for intercept)\n",
        "        X_train_scaled = np.hstack((np.ones((X_train_scaled.shape[0], 1)), X_train_scaled))\n",
        "        X_val_scaled = np.hstack((np.ones((X_val_scaled.shape[0], 1)), X_val_scaled))\n",
        "\n",
        "        # Fit the model on training data\n",
        "        beta = linear_regression(X_train_scaled, y_train)\n",
        "\n",
        "        # Predicting on the validation set\n",
        "        y_pred = X_val_scaled.dot(beta)\n",
        "        # Calculating and storing the Mean Squared Error for each fold\n",
        "        cve = mean_squared_error(y_val, y_pred)\n",
        "        cve_values.append(cve)\n",
        "\n",
        "        # Predicting on the TRAINING set for the in-sample error\n",
        "        y_train_pred = X_train_scaled.dot(beta)\n",
        "        # Calculating in-sample error for this fold\n",
        "        fold_ise = mean_squared_error(y_train, y_train_pred)\n",
        "        ise_values.append(fold_ise)\n",
        "\n",
        "    e_in = np.mean(ise_values)\n",
        "    e_cv = np.mean(cve_values)\n",
        "\n",
        "    print(f\"10-Fold Linear Regression\")\n",
        "    print(f\"e_in:{e_in}\")\n",
        "    print(f\"e_cv:{e_cv}\")\n",
        "\n",
        "    return e_in, e_cv\n",
        "\n",
        "k_fold_linear_regression(X, y, k=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQqZAo_T25X7"
      },
      "source": [
        "### Part b) Adding Ridge Regression\n",
        "Enhance the previous code to include Ridge Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "bPW731Iu25X7"
      },
      "outputs": [],
      "source": [
        "def ridge_regression(X, y, alpha):\n",
        "    # Compute the weights using the closed-form solution\n",
        "    #### TO-DO #####\n",
        "\n",
        "    ones_column = np.ones((X.shape[0], 1))\n",
        "    X_eq = np.hstack((ones_column, X))\n",
        "    identity = np.eye(X_eq.shape[1])\n",
        "    identity[0][0] = 0\n",
        "\n",
        "    # Compute the weights using the closed-form solution\n",
        "    w = np.linalg.inv(X_eq.T @ X_eq + len(X_eq) * alpha * identity) @ X_eq.T @ y\n",
        "    ##############\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "tpRjN2GU25X7"
      },
      "outputs": [],
      "source": [
        "def k_fold_ridge_regression(X, y, k=10, lambdas=list(np.logspace(-5, 1, num=15))):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross-validation for ridge regression with various lambda values without polynomial transformations.\n",
        "    \"\"\"\n",
        "    best_lambda = None\n",
        "    best_error = float('inf')\n",
        "\n",
        "    kf = KFold(n_splits=k, random_state=10, shuffle=True)\n",
        "\n",
        "    for alpha in lambdas:\n",
        "        mse_values = []\n",
        "\n",
        "        for train_index, test_index in kf.split(X):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            # Scaling the features without data leakage\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "            # Training the model using the closed-form solution for ridge regression\n",
        "            beta = ridge_regression(X_train_scaled, y_train, alpha)\n",
        "\n",
        "            # Predicting on the TEST set\n",
        "            y_pred = X_test_scaled.dot(beta[1:]) + beta[0]\n",
        "\n",
        "            # Calculating and storing the Mean Squared Error for this fold\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            mse_values.append(mse)\n",
        "\n",
        "        avg_mse = np.mean(mse_values)\n",
        "        print(f\"Alpha: {alpha:.5f}, Average E_cv: {avg_mse:.5f}\")\n",
        "\n",
        "        # Check if this alpha yields a smaller average error than the current best_error\n",
        "        if avg_mse < best_error:\n",
        "            best_error = avg_mse\n",
        "            best_lambda = alpha\n",
        "\n",
        "    print(f\"best_error:{best_error}, best_lambda:{best_lambda}\")\n",
        "    return best_lambda, best_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D333mO4625X7",
        "outputId": "d8f398c9-a76d-46e8-a4f0-3306af434db4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 0.00001, Average E_cv: 23.74027\n",
            "Alpha: 0.00003, Average E_cv: 23.74017\n",
            "Alpha: 0.00007, Average E_cv: 23.73992\n",
            "Alpha: 0.00019, Average E_cv: 23.73926\n",
            "Alpha: 0.00052, Average E_cv: 23.73754\n",
            "Alpha: 0.00139, Average E_cv: 23.73323\n",
            "Alpha: 0.00373, Average E_cv: 23.72381\n",
            "Alpha: 0.01000, Average E_cv: 23.71115\n",
            "Alpha: 0.02683, Average E_cv: 23.73427\n",
            "Alpha: 0.07197, Average E_cv: 23.95989\n",
            "Alpha: 0.19307, Average E_cv: 24.85101\n",
            "Alpha: 0.51795, Average E_cv: 27.63276\n",
            "Alpha: 1.38950, Average E_cv: 34.44903\n",
            "Alpha: 3.72759, Average E_cv: 45.89593\n",
            "Alpha: 10.00000, Average E_cv: 59.77760\n",
            "best_error:23.711151123941193, best_lambda:0.01\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.01, 23.711151123941193)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ],
      "source": [
        "#Use your code to answer question b)\n",
        "#### TO-DO ####\n",
        "k_fold_ridge_regression(X, y, k=10)\n",
        "##############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biHerywW25X7"
      },
      "source": [
        "### Part c) Adding Polynomial Transformations and Ridge Regression\n",
        "Extend their code to incorporate polynomial transformations combined with Ridge Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lob8wn-i25X7",
        "outputId": "55adb542-6f0a-4e16-a065-929b88727e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1e-05, 2.6826957952797274e-05, 7.196856730011514e-05, 0.00019306977288832496, 0.0005179474679231213, 0.0013894954943731374, 0.003727593720314938, 0.01, 0.026826957952797246, 0.07196856730011514, 0.19306977288832497, 0.5179474679231213, 1.389495494373136, 3.727593720314938, 10.0]\n",
            "Degree: 2, Alpha: 0.00001, Average E_cv: 14.19192\n",
            "Degree: 2, Alpha: 0.00003, Average E_cv: 14.17653\n",
            "Degree: 2, Alpha: 0.00007, Average E_cv: 14.12939\n",
            "Degree: 2, Alpha: 0.00019, Average E_cv: 14.00923\n",
            "Degree: 2, Alpha: 0.00052, Average E_cv: 13.74932\n",
            "Degree: 2, Alpha: 0.00139, Average E_cv: 13.28898\n",
            "Degree: 2, Alpha: 0.00373, Average E_cv: 12.67617\n",
            "Degree: 2, Alpha: 0.01000, Average E_cv: 12.13625\n",
            "Degree: 2, Alpha: 0.02683, Average E_cv: 11.92398\n",
            "Degree: 2, Alpha: 0.07197, Average E_cv: 12.24760\n",
            "Degree: 2, Alpha: 0.19307, Average E_cv: 13.57718\n",
            "Degree: 2, Alpha: 0.51795, Average E_cv: 16.67230\n",
            "Degree: 2, Alpha: 1.38950, Average E_cv: 22.03917\n",
            "Degree: 2, Alpha: 3.72759, Average E_cv: 30.63089\n",
            "Degree: 2, Alpha: 10.00000, Average E_cv: 43.39668\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.026826957952797246, 2, 11.92398392723783)"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ],
      "source": [
        "def k_fold_poly_ridge(X, y, k=10, lambdas=list(np.logspace(-5, 1, num=15)), degrees=[3]):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross-validation for ridge regression with various lambda values and polynomial transformations.\n",
        "    \"\"\"\n",
        "    best_lambda = None\n",
        "    best_degree = None\n",
        "    best_error = float('inf')\n",
        "    print(lambdas)\n",
        "    kf = KFold(n_splits=k, random_state=10, shuffle=True)\n",
        "\n",
        "    for degree in degrees:\n",
        "        poly = PolynomialFeatures(degree=degree)\n",
        "\n",
        "        for alpha in lambdas:\n",
        "            mse_values = []\n",
        "\n",
        "            for train_index, test_index in kf.split(X):\n",
        "                X_train, X_test = X[train_index], X[test_index]\n",
        "                y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "                # Scaling the features without data leakage\n",
        "                scaler = StandardScaler()\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                # Applying polynomial transformation\n",
        "                X_poly_train = poly.fit_transform(X_train_scaled)\n",
        "                X_poly_test = poly.transform(X_test_scaled)\n",
        "\n",
        "                # Training the model using the closed-form solution for ridge regression\n",
        "                beta = ridge_regression(X_poly_train, y_train, alpha)\n",
        "\n",
        "                # Predicting on the TEST set\n",
        "                y_pred = X_poly_test.dot(beta[1:]) + beta[0]\n",
        "\n",
        "                # Calculating and storing the Mean Squared Error for this fold\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                mse_values.append(mse)\n",
        "\n",
        "            avg_mse = np.mean(mse_values)\n",
        "            print(f\"Degree: {degree}, Alpha: {alpha:.5f}, Average E_cv: {avg_mse:.5f}\")\n",
        "\n",
        "            # Check if this combination of degree and alpha yields a smaller average error than the current best_error\n",
        "            if avg_mse < best_error:\n",
        "                best_error = avg_mse\n",
        "                best_lambda = alpha\n",
        "                best_degree = degree\n",
        "\n",
        "    return best_lambda, best_degree, best_error\n",
        "\n",
        "k_fold_poly_ridge(X, y, k=10, lambdas=list(np.logspace(-5, 1, num=15)), degrees=[2])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def k_fold_poly_ridge(X, y, degree=2, k=10, lambdas=list(np.logspace(-5, 1, num=15))):\n",
        "    best_lambda = None\n",
        "    best_error = float('inf')\n",
        "    N = len(X)\n",
        "\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "\n",
        "    kf = KFold(n_splits=k, random_state=10, shuffle=True)\n",
        "\n",
        "    for alpha in lambdas:\n",
        "        total_mse = 0\n",
        "        total_in_sample_mse = 0\n",
        "\n",
        "        for train_index, test_index in kf.split(X_poly):\n",
        "            X_train, X_test = X_poly[train_index], X_poly[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            # Scaling the features without data leakage\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "            # Training the model using the closed-form solution for ridge regression\n",
        "            beta = ridge_regression(X_train_scaled, y_train, alpha)\n",
        "\n",
        "            # Predicting on the TEST set\n",
        "            y_pred = X_test_scaled.dot(beta[1:])+ beta[0]\n",
        "            mse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "            total_mse += mse ** 2 * len(test_index)  # Accumulating squared errors\n",
        "\n",
        "            # Calculating in-sample error\n",
        "            y_train_pred = X_train_scaled.dot(beta[1:])+ beta[0]\n",
        "            in_sample_mse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
        "            total_in_sample_mse += in_sample_mse ** 2 * len(train_index)  # Accumulating squared errors\n",
        "\n",
        "        e_cv = total_mse / N  # Calculating cross-validation error\n",
        "        e_in = total_in_sample_mse / N  # Calculating in-sample error\n",
        "\n",
        "        print(f\"Degree: {degree}, Alpha: {alpha:.5f}, E_in: {e_in:.5f}, E_cv: {e_cv:.5f}\")\n",
        "\n",
        "        # Check if this combination of degree and alpha yields a smaller average error than the current best_error\n",
        "        if e_cv < best_error:\n",
        "            best_error = e_cv\n",
        "            best_lambda = alpha\n",
        "\n",
        "    return best_lambda, best_error\n",
        "\n",
        "best_lambda_poly, best_error_poly = k_fold_poly_ridge(X, y, degree=2, k=10, lambdas=list(np.logspace(-5, 1, num=15)))\n",
        "print(f\"Best Lambda with Polynomial Transformation: {best_lambda_poly}, Best E_cv: {best_error_poly:.5f}\")\n"
      ],
      "metadata": {
        "id": "p59RO0-8nYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68185704-bf1b-42fb-849e-e9fd0ea14515"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Degree: 2, Alpha: 0.00001, E_in: 51.97156, E_cv: 14.00488\n",
            "Degree: 2, Alpha: 0.00003, E_in: 52.36401, E_cv: 13.80012\n",
            "Degree: 2, Alpha: 0.00007, E_in: 53.32126, E_cv: 13.52200\n",
            "Degree: 2, Alpha: 0.00019, E_in: 55.27321, E_cv: 13.20981\n",
            "Degree: 2, Alpha: 0.00052, E_in: 58.59941, E_cv: 12.96272\n",
            "Degree: 2, Alpha: 0.00139, E_in: 63.43724, E_cv: 12.85781\n",
            "Degree: 2, Alpha: 0.00373, E_in: 70.12868, E_cv: 12.88773\n",
            "Degree: 2, Alpha: 0.01000, E_in: 79.51064, E_cv: 13.14772\n",
            "Degree: 2, Alpha: 0.02683, E_in: 93.64010, E_cv: 14.00832\n",
            "Degree: 2, Alpha: 0.07197, E_in: 115.59037, E_cv: 15.95248\n",
            "Degree: 2, Alpha: 0.19307, E_in: 142.47135, E_cv: 18.55947\n",
            "Degree: 2, Alpha: 0.51795, E_in: 169.13159, E_cv: 21.09848\n",
            "Degree: 2, Alpha: 1.38950, E_in: 199.52394, E_cv: 23.97442\n",
            "Degree: 2, Alpha: 3.72759, E_in: 248.76167, E_cv: 28.97993\n",
            "Degree: 2, Alpha: 10.00000, E_in: 333.45412, E_cv: 38.07114\n",
            "Best Lambda with Polynomial Transformation: 0.0013894954943731374, Best E_cv: 12.85781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Apply the polynomial transformation with degree 2 to the entire dataset\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# 2. Fit the Ridge Regression model to the entire dataset\n",
        "beta_best = ridge_regression(X_poly, y, 0.0013894954943731374)\n",
        "\n",
        "# 3. Scale and transform the given features\n",
        "test_data = np.array([0.1,11,7,0,0.4,6,70,4,6,300,16,360,10]).reshape(1, -1)\n",
        "test_data_poly = poly.transform(test_data)\n",
        "\n",
        "predicted_price = test_data_poly.dot(beta_best[1:])+ beta_best[0]\n",
        "\n",
        "print(f\"Predicted average house price for the given features: {predicted_price[0][0]:.2f}\")\n",
        "len(beta_best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmkGn30xbMP3",
        "outputId": "30aa0a57-65cb-40a7-93c2-d5c621b5ad1d"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted average house price for the given features: 23.19\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output_file_with_insights():\n",
        "    with open(\"output.txt\", \"w\") as f:\n",
        "\n",
        "        # Part (a)\n",
        "        f.write(\"a) List of E_cv and average E_in for different λ values:\\n\")\n",
        "        for lambda_val in [0] + list(np.logspace(-5, 1, num=15)):\n",
        "            e_in, e_cv = k_fold_ridge_regression(X, y, k=10, lambdas=[lambda_val])\n",
        "            f.write(f\"Lambda: {lambda_val}, E_in: {e_in}, E_cv: {e_cv}\\n\")\n",
        "\n",
        "        # Part (b)\n",
        "        best_lambda_poly, best_error_poly = k_fold_poly_ridge(X, y, degree=2, k=10, lambdas=list(np.logspace(-5, 1, num=15)))\n",
        "\n",
        "        # 1. Apply the polynomial transformation with degree 2 to the entire dataset\n",
        "        poly = PolynomialFeatures(degree=2)\n",
        "        X_poly = poly.fit_transform(X)\n",
        "\n",
        "        # 2. Fit the Ridge Regression model to the entire dataset\n",
        "        beta_best = ridge_regression(X_poly, y, best_lambda_poly)\n",
        "\n",
        "        # 3. Scale and transform the given features\n",
        "        test_data = np.array([0.1, 11, 7, 0, 0.4, 6, 70, 4, 6, 300, 16, 360, 10]).reshape(1, -1)\n",
        "        test_data_poly = poly.transform(test_data)\n",
        "        predicted_price = test_data_poly.dot(beta_best[1:]) + beta_best[0]\n",
        "\n",
        "        f.write(\"\\nb) Model Selection and Prediction:\\n\")\n",
        "        f.write(\"Given a choice, I would select the Ridge Regression model with a Polynomial Transformation. \")\n",
        "        f.write(f\"\\nSpecified Parameters:\\n{beta_best}\")\n",
        "        # Part (c)\n",
        "        insights = \"\"\"\n",
        "        \\nc) Insights:\n",
        "\n",
        "        1. The Ridge Regression model's performance indicates the potential presence of multicollinearity or other complex relationships within the features of the Boston Housing dataset.\n",
        "\n",
        "        2. The selection of the optimal λ demonstrates the importance of regularization in preventing overfitting, especially when the dataset has many features or when polynomial transformations are used.\n",
        "\n",
        "        3. Polynomial transformations helped to improve the model's performance, suggesting that some relationships between the variables are non-linear in nature.\n",
        "\n",
        "        4. The performance difference between various λ values stresses the importance of hyperparameter tuning in machine learning models.\n",
        "\n",
        "        5. It's essential to scale features, especially when using Ridge Regression, to ensure that each feature's weight is determined without the influence of its scale.\n",
        "\n",
        "        In conclusion, the experiments indicate the significance of feature engineering, model selection, and hyperparameter tuning in determining the performance of regression models.\n",
        "        \"\"\"\n",
        "        f.write(insights)\n",
        "\n",
        "generate_output_file_with_insights()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwqjNc1a1BAc",
        "outputId": "551e99d7-effc-4423-8b59-aff2c9e55b9b"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 0.00000, Average E_cv: 23.74032\n",
            "best_error:23.740321100278067, best_lambda:0\n",
            "Alpha: 0.00001, Average E_cv: 23.74027\n",
            "best_error:23.74026574931244, best_lambda:1e-05\n",
            "Alpha: 0.00003, Average E_cv: 23.74017\n",
            "best_error:23.740172756333124, best_lambda:2.6826957952797274e-05\n",
            "Alpha: 0.00007, Average E_cv: 23.73992\n",
            "best_error:23.739924186815507, best_lambda:7.196856730011514e-05\n",
            "Alpha: 0.00019, Average E_cv: 23.73926\n",
            "best_error:23.739263817150167, best_lambda:0.00019306977288832496\n",
            "Alpha: 0.00052, Average E_cv: 23.73754\n",
            "best_error:23.73753826322511, best_lambda:0.0005179474679231213\n",
            "Alpha: 0.00139, Average E_cv: 23.73323\n",
            "best_error:23.733230498055857, best_lambda:0.0013894954943731374\n",
            "Alpha: 0.00373, Average E_cv: 23.72381\n",
            "best_error:23.72381173190798, best_lambda:0.003727593720314938\n",
            "Alpha: 0.01000, Average E_cv: 23.71115\n",
            "best_error:23.711151123941193, best_lambda:0.01\n",
            "Alpha: 0.02683, Average E_cv: 23.73427\n",
            "best_error:23.7342697287092, best_lambda:0.026826957952797246\n",
            "Alpha: 0.07197, Average E_cv: 23.95989\n",
            "best_error:23.95988632873602, best_lambda:0.07196856730011514\n",
            "Alpha: 0.19307, Average E_cv: 24.85101\n",
            "best_error:24.851014016967223, best_lambda:0.19306977288832497\n",
            "Alpha: 0.51795, Average E_cv: 27.63276\n",
            "best_error:27.63275755329237, best_lambda:0.5179474679231213\n",
            "Alpha: 1.38950, Average E_cv: 34.44903\n",
            "best_error:34.44903324740632, best_lambda:1.389495494373136\n",
            "Alpha: 3.72759, Average E_cv: 45.89593\n",
            "best_error:45.89593159305474, best_lambda:3.727593720314938\n",
            "Alpha: 10.00000, Average E_cv: 59.77760\n",
            "best_error:59.777595718066706, best_lambda:10.0\n",
            "Degree: 2, Alpha: 0.00001, E_in: 51.97156, E_cv: 14.00488\n",
            "Degree: 2, Alpha: 0.00003, E_in: 52.36401, E_cv: 13.80012\n",
            "Degree: 2, Alpha: 0.00007, E_in: 53.32126, E_cv: 13.52200\n",
            "Degree: 2, Alpha: 0.00019, E_in: 55.27321, E_cv: 13.20981\n",
            "Degree: 2, Alpha: 0.00052, E_in: 58.59941, E_cv: 12.96272\n",
            "Degree: 2, Alpha: 0.00139, E_in: 63.43724, E_cv: 12.85781\n",
            "Degree: 2, Alpha: 0.00373, E_in: 70.12868, E_cv: 12.88773\n",
            "Degree: 2, Alpha: 0.01000, E_in: 79.51064, E_cv: 13.14772\n",
            "Degree: 2, Alpha: 0.02683, E_in: 93.64010, E_cv: 14.00832\n",
            "Degree: 2, Alpha: 0.07197, E_in: 115.59037, E_cv: 15.95248\n",
            "Degree: 2, Alpha: 0.19307, E_in: 142.47135, E_cv: 18.55947\n",
            "Degree: 2, Alpha: 0.51795, E_in: 169.13159, E_cv: 21.09848\n",
            "Degree: 2, Alpha: 1.38950, E_in: 199.52394, E_cv: 23.97442\n",
            "Degree: 2, Alpha: 3.72759, E_in: 248.76167, E_cv: 28.97993\n",
            "Degree: 2, Alpha: 10.00000, E_in: 333.45412, E_cv: 38.07114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRzIqNS-3dPT"
      },
      "execution_count": 174,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}