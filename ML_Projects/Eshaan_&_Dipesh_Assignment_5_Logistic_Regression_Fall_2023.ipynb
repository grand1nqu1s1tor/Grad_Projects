{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoFPTC2diw-_"
      },
      "source": [
        "# Programming Assignment 5 - Logistic Regression\n",
        "\n",
        "## In this assignment:\n",
        "\n",
        "You'll employ gradient ascent to determine weights for a logistic regression problem focused on diagnosing breast cancer.\n",
        "\n",
        "### Dataset Overview:\n",
        "\n",
        "The **Breast Cancer Wisconsin dataset** is a widely-recognized collection of features manually recorded by physicians from fine needle aspiration samples. The primary objective is to determine whether the cells are benign or malignant based on these features.\n",
        "\n",
        "**Dataset details:** [Breast Cancer Wisconsin dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin)\n",
        "\n",
        "Each sample from the dataset is derived from a digitized image of a fine needle aspirate (FNA) of a breast mass. These images are processed to extract characteristics of cell nuclei, which are instrumental in the diagnostic process.\n",
        "\n",
        "### Features:\n",
        "\n",
        "The dataset consists of ten real-valued features that provide various measurements related to the cell nucleus:\n",
        "\n",
        "1. **Radius:** Mean of distances from the center to points on the perimeter.\n",
        "2. **Texture:** Standard deviation of gray-scale values.\n",
        "3. **Perimeter**\n",
        "4. **Area**\n",
        "5. **Smoothness:** Local variation in radius lengths.\n",
        "6. **Compactness:** \\( \\frac{\\text{perimeter}^2}{\\text{area}} - 1.0 \\)\n",
        "7. **Concavity:** Severity of concave portions of the contour.\n",
        "8. **Concave Points:** Number of concave portions of the contour.\n",
        "9. **Symmetry**\n",
        "10. **Fractal Dimension:** \"Coastline approximation\" - 1.\n",
        "\n",
        "### Task:\n",
        "\n",
        "Your mission is to use logistic regression on the provided features to predict whether a tumor is benign or malignant. Successfully doing so can greatly aid in early diagnosis, ultimately leading to saved lives.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5urpIQFiw_C"
      },
      "source": [
        "## Step 1:  Getting, preprocessing, and understanding the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktdDJZ_giw_D"
      },
      "source": [
        "### Importing the standard libraries\n",
        "\n",
        "### Essential Libraries\n",
        "\n",
        "- **NumPy**: A library for numerical operations in Python.\n",
        "- **Matplotlib**: Provides a way to visualize data.\n",
        "\n",
        "### Scikit-learn Utilities\n",
        "\n",
        "- **load_breast_cancer**: Dataset included in Scikit-learn for breast cancer classification.\n",
        "- **preprocessing**: Contains methods for preparing data before applying learning algorithms.\n",
        "- **train_test_split**: A utility function to split data into training and testing sets.\n",
        "\n",
        "> **Note**: Using the `%matplotlib inline` command ensures that Matplotlib visualizations are rendered directly within the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "FQsv9kh_iw_D"
      },
      "outputs": [],
      "source": [
        "# Essential libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn utilities\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set up matplotlib for inline display\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJbgdDOIiw_F"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "oGjU_tCKiw_G"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "y = cancer.target\n",
        "X = cancer.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bXuDs9wiw_G",
        "outputId": "3546147c-0e47-4823-a15a-bd0403d61717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30)\n",
            "(569,)\n"
          ]
        }
      ],
      "source": [
        "# Printing the shape of data (X) and target (Y) values\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBm33ccCiw_G"
      },
      "source": [
        "### Data Pre-Processing\n",
        "\n",
        "### Splitting the Data\n",
        "We divide our dataset into a training set and a testing set:\n",
        "- **Training Set**: 75%\n",
        "- **Testing Set**: 25%\n",
        "\n",
        "Use the `train_test_split` function to achieve this split:\n",
        "- Assign results to: `X_train`, `X_test`, `y_train`, `y_test`\n",
        "- Set `random_state` to 42 to ensure reproducibility.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "_XZW4yD9iw_G"
      },
      "outputs": [],
      "source": [
        "# Split the data using train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiudGq5ciw_H"
      },
      "source": [
        "### Scaling the Data Using Standard Scaler\n",
        "\n",
        "Since we are using gradient ascent, it's important to scale our data to ensure faster convergence. One of the most common methods to scale data is to use the `Standard Scaler`.\n",
        "\n",
        "The `Standard Scaler` normalizes the features by subtracting the mean and scaling to unit variance.\n",
        "\n",
        "Using `Standard Scaler`, each feature will have a mean of 0 and a standard deviation of 1 post-scaling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "cw1946Aziw_H"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQC-I7jUiw_I",
        "outputId": "383d247e-89ec-4eb4-dc12-9152e2da6823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_train: (426, 30)\n",
            "Shape of y_train: (426,)\n"
          ]
        }
      ],
      "source": [
        "# TODO - Print the shape of x_train and y_train\n",
        "print(\"Shape of x_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "##\n",
        " # When you print the shape of x_train, it should print (426, 30)\n",
        " # When you print the shape of y_train, it should print (426,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh0e09Gdiw_I"
      },
      "source": [
        "#### Adding a Bias Term to $X_{\\text{train}}$ and $X_{\\text{test}}$\n",
        "To account for the intercept term in our logistic regression model, we augment our feature matrices with a column of ones. This is often referred to as the bias term.\n",
        "\n",
        "Given our original matrix:\n",
        "$$X_{\\text{train}}=\\left[\\begin{matrix}\n",
        "x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
        "x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
        "\\vdots & \\vdots &\\ddots & \\vdots \\\\\n",
        "x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
        "\\end{matrix}\\right]$$\n",
        "\n",
        "We add a column of ones:\n",
        "$$ X_{\\text{train}}=\\left[\\begin{matrix}\n",
        "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
        "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
        "\\vdots & \\vdots &\\vdots &\\ddots & \\vdots \\\\\n",
        "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
        "\\end{matrix}\\right]$$\n",
        "\n",
        "Similarly, we augment $X_{\\text{test}}$ with a column of ones. This allows our algorithm to learn an intercept term without needing special handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCYoh3ceiw_J",
        "outputId": "45c4aa2a-8037-4c2e-f64a-12078090279e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The trainng data has dimensions:  (426, 31) . The testing data has dimensions:  (143, 31)\n",
            "[[ 1.         -0.34913849 -1.43851335 -0.41172595 -0.39047943 -1.86366229\n",
            "  -1.26860704 -0.82617052 -0.95286585 -1.72936805 -0.9415409  -0.86971355\n",
            "  -1.35865347 -0.83481506 -0.57230673 -0.74586846 -0.65398319 -0.52583524\n",
            "  -0.94677147 -0.53781728 -0.63449458 -0.54268486 -1.65565452 -0.58986401\n",
            "  -0.52555985 -1.51066925 -0.89149994 -0.75021715 -0.91671059 -0.92508585\n",
            "  -0.80841115]\n",
            " [ 1.         -0.20468665  0.31264011 -0.13367256 -0.27587995  1.07807258\n",
            "   0.86354605  0.72631375  0.89844062  1.17876963  1.47437716 -0.04022275\n",
            "  -0.50962253  0.10947722 -0.13472838 -0.52489487 -0.14934475  0.07460028\n",
            "   0.23747244 -0.43028253  0.08289146  0.04148684  0.68989862  0.19412774\n",
            "  -0.05193356  1.12941497  0.92394223  1.22221738  1.43655962  1.14955889\n",
            "   1.56911143]]\n"
          ]
        }
      ],
      "source": [
        "# Appending a column of ones to x_train\n",
        "\n",
        "# Step 1: Create a column vector of ones (i.e. a vector of shape N',1)\n",
        "ones = np.ones(X_train.shape[0]).reshape((X_train.shape[0], 1))\n",
        "\n",
        "# Step 2: Append a column of ones in the beginning of x_train\n",
        "X_train = np.hstack((ones, X_train))\n",
        "\n",
        "\n",
        "# Now do the same for the test data\n",
        "# Step 1: Create a column vector of ones (i.e. a vector of shape N\",1)\n",
        "ones = np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1))\n",
        "\n",
        "# Stemp 2: Append a column of ones in the beginning of x_test\n",
        "X_test = np.hstack((ones, X_test))\n",
        "\n",
        "\n",
        "# We can check that everything worked correctly by:\n",
        "# Printing out the new dimensions\n",
        "print(\"The trainng data has dimensions: \", X_train.shape, \". The testing data has dimensions: \",X_test.shape)\n",
        "\n",
        "# Looking at the first two rows of X_train to check everything worked as expected\n",
        "print(X_train[0:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOxpb3xCiw_J"
      },
      "source": [
        "### Reshaping the Target Vectors\n",
        "\n",
        "due to the broadcasting feature in libraries like NumPy, if we're not careful with the shapes of our matrices, we might unintentionally compute the outer product instead of the desired inner product. This can lead to unexpected results and potential bugs in the algorithm. Reshaping the target vectors into 2D arrays helps prevent such issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "IobteEW0iw_J"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.reshape(y_train.shape[0],1)\n",
        "y_test = y_test.reshape((y_test.shape[0],1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfwRPOqsiw_K"
      },
      "source": [
        "### Understanding the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izRuFzqOiw_K",
        "outputId": "e2c5a3c6-dffd-40c7-9365-02e103fea543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 569\n",
            "\n",
            "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            "    :Attribute Information:\n",
            "        - radius (mean of distances from center to points on the perimeter)\n",
            "        - texture (standard deviation of gray-scale values)\n",
            "        - perimeter\n",
            "        - area\n",
            "        - smoothness (local variation in radius lengths)\n",
            "        - compactness (perimeter^2 / area - 1.0)\n",
            "        - concavity (severity of concave portions of the contour)\n",
            "        - concave points (number of concave portions of the contour)\n",
            "        - symmetry\n",
            "        - fractal dimension (\"coastline approximation\" - 1)\n",
            "\n",
            "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
            "        worst/largest values) of these features were computed for each image,\n",
            "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
            "        10 is Radius SE, field 20 is Worst Radius.\n",
            "\n",
            "        - class:\n",
            "                - WDBC-Malignant\n",
            "                - WDBC-Benign\n",
            "\n",
            "    :Summary Statistics:\n",
            "\n",
            "    ===================================== ====== ======\n",
            "                                           Min    Max\n",
            "    ===================================== ====== ======\n",
            "    radius (mean):                        6.981  28.11\n",
            "    texture (mean):                       9.71   39.28\n",
            "    perimeter (mean):                     43.79  188.5\n",
            "    area (mean):                          143.5  2501.0\n",
            "    smoothness (mean):                    0.053  0.163\n",
            "    compactness (mean):                   0.019  0.345\n",
            "    concavity (mean):                     0.0    0.427\n",
            "    concave points (mean):                0.0    0.201\n",
            "    symmetry (mean):                      0.106  0.304\n",
            "    fractal dimension (mean):             0.05   0.097\n",
            "    radius (standard error):              0.112  2.873\n",
            "    texture (standard error):             0.36   4.885\n",
            "    perimeter (standard error):           0.757  21.98\n",
            "    area (standard error):                6.802  542.2\n",
            "    smoothness (standard error):          0.002  0.031\n",
            "    compactness (standard error):         0.002  0.135\n",
            "    concavity (standard error):           0.0    0.396\n",
            "    concave points (standard error):      0.0    0.053\n",
            "    symmetry (standard error):            0.008  0.079\n",
            "    fractal dimension (standard error):   0.001  0.03\n",
            "    radius (worst):                       7.93   36.04\n",
            "    texture (worst):                      12.02  49.54\n",
            "    perimeter (worst):                    50.41  251.2\n",
            "    area (worst):                         185.2  4254.0\n",
            "    smoothness (worst):                   0.071  0.223\n",
            "    compactness (worst):                  0.027  1.058\n",
            "    concavity (worst):                    0.0    1.252\n",
            "    concave points (worst):               0.0    0.291\n",
            "    symmetry (worst):                     0.156  0.664\n",
            "    fractal dimension (worst):            0.055  0.208\n",
            "    ===================================== ====== ======\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
            "\n",
            "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
            "\n",
            "    :Donor: Nick Street\n",
            "\n",
            "    :Date: November, 1995\n",
            "\n",
            "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
            "https://goo.gl/U2Uwz2\n",
            "\n",
            "Features are computed from a digitized image of a fine needle\n",
            "aspirate (FNA) of a breast mass.  They describe\n",
            "characteristics of the cell nuclei present in the image.\n",
            "\n",
            "Separating plane described above was obtained using\n",
            "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
            "Construction Via Linear Programming.\" Proceedings of the 4th\n",
            "Midwest Artificial Intelligence and Cognitive Science Society,\n",
            "pp. 97-101, 1992], a classification method which uses linear\n",
            "programming to construct a decision tree.  Relevant features\n",
            "were selected using an exhaustive search in the space of 1-4\n",
            "features and 1-3 separating planes.\n",
            "\n",
            "The actual linear program used to obtain the separating plane\n",
            "in the 3-dimensional space is that described in:\n",
            "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
            "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
            "Optimization Methods and Software 1, 1992, 23-34].\n",
            "\n",
            "This database is also available through the UW CS ftp server:\n",
            "\n",
            "ftp ftp.cs.wisc.edu\n",
            "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
            "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
            "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
            "     San Jose, CA, 1993.\n",
            "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
            "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
            "     July-August 1995.\n",
            "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
            "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
            "     163-171.\n"
          ]
        }
      ],
      "source": [
        "# Read through the description of the dataset by uncommenting the line of code below\n",
        "print(cancer.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "szAiziJHiw_K"
      },
      "outputs": [],
      "source": [
        "# You can add your own code here to better understand the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yjWeLsLiw_K"
      },
      "source": [
        "\n",
        "# Step 2: Fitting the model\n",
        "\n",
        "## Implementing Logistic Regression Using Gradient Ascent\n",
        "\n",
        "\n",
        "You will perform the following steps:\n",
        "* write the sigmoid function $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
        "* initialize ${\\bf w}$\n",
        "* prediction: write the function to compute the probability of every example in $X$ belonging to class one\n",
        "* write the log likelihood function (see lecture notes for the formula)\n",
        "* write the gradient ascent algorithm\n",
        "* plot the likelihood v/s the number of iterations\n",
        "* predict the class label (i.e. $0,1$) for every example in $X$ for a given ${\\bf w}$ and $t$\n",
        "* Evaluate your hypothesis by using your hypothesis to predict the label of the examples in the test set.  Using these predicted value you will then determine the precision, recall and F1 score of the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyDal_wRiw_L"
      },
      "source": [
        "\n",
        "\n",
        "### Sigmoid($z$)\n",
        "The first function you will write is sigmoid($z$)\n",
        "\n",
        "sigmoid($z$) takes as input a column vector of real numbers, $z^T = [z_1, z_2, ..., z_{N'}]$, where $N'$ is the number of  examples\n",
        "\n",
        "It should produce as output a column vector $\\left[\\frac{1}{1+e^{-z_1}},\\frac{1}{1+e^{-z_2}},...,\\frac{1}{1+e^{-z_{N'}}}\\right]^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "O9stsL3Ziw_L"
      },
      "outputs": [],
      "source": [
        "# TODO - Write the sigmoid function (z can be a scalar or a vector)\n",
        "def sigmoid(z):\n",
        "    ## TODO\n",
        "\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UEE5fxOiw_L",
        "outputId": "3722dc65-d613-4cb7-b40d-ab927bd94b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ],
      "source": [
        "# VERIFY - Sigmoid of 0 should be equal to 0.5\n",
        "print(sigmoid(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6m8XkQQiw_L"
      },
      "source": [
        "### Initializing ${\\bf w}$\n",
        "For testing the next functions, we create a coefficient vector, ${\\bf w}$.\n",
        "We will initialize the coeffients to be $0$, i.e. ${\\bf w}^T = [0,0,\\ldots ,0]$ (We could have initialized ${\\bf w}$ with any values.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtYoQExQiw_L",
        "outputId": "c9196ec5-c259-4715-dd8f-565100ea7d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31,)\n"
          ]
        }
      ],
      "source": [
        "# Initialize parameters w\n",
        "## TODO\n",
        "# Initialize the coefficient vector w with zeros\n",
        "w = np.zeros(31)\n",
        "##\n",
        "print(w.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYx8CRYziw_L"
      },
      "source": [
        "### Prediction Function\n",
        "Complete the `hypothesis` function to compute the probability that each example in \\(X\\) belongs to class one. Specifically, it calculates:\n",
        "\n",
        "$$\\hat{\\bf y}=\\sigma(X{\\bf w})$$\n",
        "\n",
        "For a single example represented by the design matrix:\n",
        "\n",
        "$$X=[1,x_1,x_2,\\ldots,x_d]$$\n",
        "\n",
        "and the corresponding weight vector:\n",
        "\n",
        "$${\\bf w}^T=[w_0,w_1,\\ldots, w_d]$$\n",
        "\n",
        "The function returns the logistic regression prediction:\n",
        "\n",
        "$$h({\\bf x})=\\frac{1}{1+e^{-\\left({w_{0}\\cdot 1 +w_1\\cdot x_1+\\cdots +w_d\\cdot x_d}\\right)}}$$\n",
        "\n",
        "Given a matrix with $N'$ examples:\n",
        "\n",
        "$$X=\\left[\\begin{matrix}\n",
        "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
        "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
        "\\vdots & \\vdots &\\vdots &\\ddots & \\vdots \\\\\n",
        "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
        "\\end{matrix}\\right]$$\n",
        "\n",
        "with the same weight vector, the function will return:\n",
        "\n",
        "$$[h({\\bf x}^{(1)}),h({\\bf x}^{(2)}),\\ldots, h({\\bf x}^{(N')})]^T$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Fcu7Apdhiw_M"
      },
      "outputs": [],
      "source": [
        "# predict the probability that a patient has cancer\n",
        "# TODO - Write the hypothesis function\n",
        "def hypothesis(X , w):\n",
        "    #TODO\n",
        "    z = np.dot(X, w)\n",
        "\n",
        "    # Calculate the logistic regression prediction using the sigmoid function\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    return h\n",
        "    ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhwkRjsHiw_M"
      },
      "source": [
        "Before moving on, do a quick check that your function can accpet a matrix as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I7ADnUciw_M",
        "outputId": "525245ff-bb77-432c-ef59-a7b221877177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(426,)\n",
            "(426, 1)\n"
          ]
        }
      ],
      "source": [
        "# Compute y_hat using our training examples and w (w is still set to zero).\n",
        "# This is just a preliminary test of the hypotheis function\n",
        "yhat = hypothesis(X_train, w)\n",
        "\n",
        "# print the sizes of yhat and y as a first check that the function performed correctly\n",
        "print(yhat.shape) # this should return (426, 1)\n",
        "print(y_train.shape) # this should return (426,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiPPsYNviw_M"
      },
      "source": [
        "### Log-Likelihood Function\n",
        "\n",
        "Write the function to calculate the log-likelihood:\n",
        "\n",
        "$$\n",
        "\\ell({\\bf w})= \\sum_{i=1}^{N'} y^{(i)} \\ln(h({\\bf x}^{(i)})) + (1 - y^{(i)}) \\ln(1 - h({\\bf x}^{(i)}))\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- **Input**:\n",
        "  - Design matrix with $N'$ examples:\n",
        "    \n",
        "   $$\n",
        "    X = \\left[\\begin{array}{cccc}\n",
        "    1 & x^{(1)}_1 & \\ldots & x^{(1)}_d \\\\\n",
        "    1 & x^{(2)}_1 & \\ldots & x^{(2)}_d \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    1 & x^{(N')}_1 & \\ldots & x^{(N')}_d \\\\\n",
        "    \\end{array}\\right]\n",
        "   $$\n",
        "    \n",
        "  - Column vector of labels for $X$:\n",
        "    \n",
        "  $$\n",
        "    {\\bf y}^T = [y^{(1)}, y^{(2)}, \\ldots, y^{(N')}]\n",
        "   $$\n",
        "  \n",
        "- **Output**:\n",
        "  - Log-likelihood value: $\\ell({\\bf w})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "nyFAVcp2iw_N"
      },
      "outputs": [],
      "source": [
        "# TODO - Write the log likelihood function\n",
        "def log_likelihood(X , y , w):\n",
        "    ##TODO\n",
        "    h = hypothesis(X, w)\n",
        "\n",
        "    likelihood = np.dot(y.T, np.log(h)) + np.dot((1 - y.T), np.log(1 - h))\n",
        "\n",
        "    log_likelihood = np.sum(likelihood)\n",
        "\n",
        "    ##\n",
        "    return log_likelihood # you should return a real number, not a list containing a real number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fKGWxZgiw_N"
      },
      "source": [
        "Before moving on, do a quick check of your log_likelihood function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5oThzEBiw_N",
        "outputId": "098fb7a9-d937-4820-cc64-014bb2940fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-295.2806989185367\n"
          ]
        }
      ],
      "source": [
        "# VERIFY - The value should be equal to -295.2806989185367.\n",
        "print(log_likelihood(X_train, y_train, w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIuhlsi9iw_N"
      },
      "source": [
        "# Gradient Ascent\n",
        "Now write the code to perform gradient ascent.  You will use the update rule from the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "AEm6YSo-iw_N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def Logistic_Regression_Gradient_Ascent(X, y, learning_rate, num_iters):\n",
        "    # We assume X has been augmented with a column of ones\n",
        "\n",
        "    # Initiating list to store values of log-likelihood after 100 iterations\n",
        "    log_likelihood_values = []\n",
        "\n",
        "    # Initialize w to be a zero vector of shape (X.shape[1], 1)\n",
        "    w = np.zeros((X.shape[1], 1))\n",
        "\n",
        "    # Initialize N to the number of training examples\n",
        "    N = X.shape[0]\n",
        "\n",
        "    # Gradient Ascent - local optimization technique\n",
        "    for i in range(num_iters):\n",
        "        # Calculate the predicted probabilities using the sigmoid function\n",
        "        h = 1 / (1 + np.exp(-np.dot(X, w)))\n",
        "\n",
        "        # Calculate the gradient of the log-likelihood with respect to w\n",
        "        gradient = np.dot(X.T, (y - h))\n",
        "\n",
        "        # Update w using the gradient ascent formula\n",
        "        w += (learning_rate / N) * gradient\n",
        "\n",
        "        # Every 100 iterations, store the log_likelihood for the current w\n",
        "        if (i % 100) == 0:\n",
        "            curr_log_likelihood = log_likelihood(X, y, w)\n",
        "            log_likelihood_values.append(curr_log_likelihood)\n",
        "            # On your own, monitor the learning process, print the iteration number, the log-likelihood, ...\n",
        "\n",
        "    return w, log_likelihood_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDoc60uAiw_O"
      },
      "source": [
        "### After completing the code above, run the following"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coeffecient Vector w"
      ],
      "metadata": {
        "id": "hT1y9OM23fRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE12lqu0iw_O",
        "outputId": "5bab983c-4187-4506-fcc7-9e11b054c8f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient vector w:\n",
            "[[-0.1056701 ]\n",
            " [-0.03553097]\n",
            " [-0.0536691 ]\n",
            " [ 0.17221516]\n",
            " [-0.31116366]\n",
            " [-0.45434972]\n",
            " [ 2.75131896]\n",
            " [-1.30570826]\n",
            " [-3.03314808]\n",
            " [ 1.16803283]\n",
            " [-0.85636742]\n",
            " [-3.72793978]\n",
            " [ 0.72266145]\n",
            " [-0.80953786]\n",
            " [-2.47051644]\n",
            " [-0.40297532]\n",
            " [ 0.53078335]\n",
            " [ 0.03099043]\n",
            " [-1.27545053]\n",
            " [ 1.31001975]\n",
            " [ 2.1018566 ]\n",
            " [-1.64716699]\n",
            " [-2.84431473]\n",
            " [-0.20035616]\n",
            " [-1.77971341]\n",
            " [-0.16157425]\n",
            " [ 0.73553297]\n",
            " [-2.45713669]\n",
            " [-1.31520556]\n",
            " [-2.99931319]\n",
            " [-0.38232592]]\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "learning_rate = 0.5\n",
        "num_iters = 5000\n",
        "\n",
        "# Initialize coefficients to zeros\n",
        "w = np.zeros((X_train.shape[1], 1))\n",
        "\n",
        "# Perform logistic regression using gradient ascent\n",
        "w, log_likelihood_values = Logistic_Regression_Gradient_Ascent(X_train, y_train, learning_rate, num_iters)\n",
        "\n",
        "# Print the final coefficients\n",
        "print(\"Coefficient vector w:\")\n",
        "print(w)\n",
        "# # Print the log likelihoood\n",
        "# print(\"Log Likelihood:\")\n",
        "# print(log_likelihood_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBBywTYEiw_O"
      },
      "source": [
        "# Plotting Log-Likelihood v/s Number of Iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "KyzjNEs_iw_O",
        "outputId": "d3ea7b5c-013d-4761-e835-2c1f9633d10e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Run this cell to plot Likelihood v/s Number of Iterations.\n",
        "iters = np.array(range(0,num_iters,100))\n",
        "plt.plot(iters,log_likelihood_values,'.-',color='green')\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Log-Likelihood')\n",
        "plt.title(\"Log-Likelihood vs Number of Iterations.\")\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPeZxDjjiw_f"
      },
      "source": [
        "You should see the likelihood increasing as number of Iterations increase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKeMqZM7iw_f",
        "outputId": "285af38a-36fe-4ce4-aa81-df334101e0e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Likelihood:\n",
            "[-104.34561005259052, -30.613421040599732, -26.679954427411666, -24.76478769939301, -23.544913930501746, -22.668618640929342, -21.995874538657674, -21.45672990771387, -21.009472103607315, -20.62725194414171, -20.292743277182133, -19.994625484608665, -19.725219641902154, -19.47908964602271, -19.252242101930726, -19.041653725735586, -18.844978275885236, -18.66035663838307, -18.486289239717962, -18.321547742766644, -18.16511236690654, -18.0161264094924, -17.873862603144918, -17.737697795406596, -17.607093594208827, -17.481581364550586, -17.360750448842477, -17.244238809830648, -17.131725517984442, -17.022924660136155, -16.917580355470143, -16.815462643191943, -16.71636406294889, -16.620096790720158, -16.526490223816662, -16.435388931827745, -16.346650907930005, -16.26014606841489, -16.175754958663532, -16.093367631862147, -16.012882673077193, -15.93420634630247, -15.857251846063125, -15.781938638342, -15.708191878155375, -15.635941893182663, -15.565123724547389, -15.49567671723482, -15.427544153774665, -15.360672925764225]\n"
          ]
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "learning_rate = 0.5\n",
        "num_iterations = 5000\n",
        "\n",
        "# Initialize coefficients to zeros\n",
        "w = np.zeros((X_train.shape[1], 1))\n",
        "\n",
        "# Perform logistic regression using gradient ascent\n",
        "w, log_likelihood_values = Logistic_Regression_Gradient_Ascent(X_train, y_train, learning_rate, num_iterations)\n",
        "\n",
        "# # Print the final coefficients\n",
        "# print(\"Coefficient vector w:\")\n",
        "# print(w)\n",
        "# Print the log likelihoood\n",
        "print(\"Log Likelihood:\")\n",
        "print(log_likelihood_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co_m9Nf2iw_g"
      },
      "source": [
        "# Step 3: Evaluating your model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_labels(X, w):\n",
        "    probabilities = sigmoid(np.dot(X, w))\n",
        "    return [1 if prob >= 0.5 else 0 for prob in probabilities]\n",
        "\n",
        "y_pred = predict_labels(X_test, w)"
      ],
      "metadata": {
        "id": "_kTKv8iCxX-2"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi-281Kxiw_g",
        "outputId": "ff9a364b-4108-452f-cb3c-cf70b186c038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9885057471264368\n",
            "Recall: 0.9662921348314607\n",
            "F1 Score: 0.9772727272727273\n",
            "Confusion Matrix:\n",
            "[[53  1]\n",
            " [ 3 86]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Predict probabilities for the test data using the learned coefficients\n",
        "test_predictions = hypothesis(X_test, w)\n",
        "\n",
        "# Threshold the probabilities to make binary predictions\n",
        "threshold = 0.5\n",
        "test_predictions_binary = (test_predictions >= threshold).astype(int)\n",
        "\n",
        "# Calculate precision, recall, and (optional) F1 score\n",
        "precision = precision_score(y_test, test_predictions_binary)\n",
        "recall = recall_score(y_test, test_predictions_binary)\n",
        "f1 = f1_score(y_test, test_predictions_binary)\n",
        "\n",
        "# Compute and print the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions_binary)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rwrfFXziw_g"
      },
      "source": [
        "#  Experiment with different hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "oNQ4_pq4iw_g"
      },
      "outputs": [],
      "source": [
        "# Define a list of learning rates and iterations to experiment with\n",
        "learning_rates_list = [0.01, 0.1, 0.2, 0.5, 1.0]  # Different learning rates\n",
        "iterations_list = [100, 500, 1000, 2000, 5000]  # Different numbers of iterations\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "# Iterate over learning rates and iterations\n",
        "for learning_rate in learning_rates_list:\n",
        "    for num_iterations in iterations_list:\n",
        "        # Train a logistic regression model with the current hyperparameters\n",
        "        trained_weights, log_likelihood_values = Logistic_Regression_Gradient_Ascent(X_train, y_train, learning_rate, num_iterations)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        predicted_y = predict_labels(X_test, trained_weights)\n",
        "\n",
        "        # Calculate evaluation metrics such as precision, recall, and F1 score\n",
        "        precision_value = precision_score(y_test, predicted_y)\n",
        "        recall_value = recall_score(y_test, predicted_y)\n",
        "        f1_score_value = f1_score(y_test, predicted_y)\n",
        "        confusion_matrix_data = confusion_matrix(y_test, predicted_y)\n",
        "\n",
        "        # Store the evaluation metrics and hyperparameters in a dictionary\n",
        "        evaluation_metrics = {\n",
        "            'Precision': precision_value,\n",
        "            'Recall': recall_value,\n",
        "            'F1 Score': f1_score_value,\n",
        "            'Learning Rate': learning_rate,\n",
        "            'Number of Iterations': num_iterations\n",
        "        }\n",
        "        evaluation_results.append(evaluation_metrics)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Observation After Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "Ryt8Wxod1HMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the header with the desired order\n",
        "header = [\"Learning Rate\", \"Iterations\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "\n",
        "# Print the header with proper formatting\n",
        "print(f\"{header[0]:<15}{header[1]:<20}{header[2]:<15}{header[3]:<15}{header[4]:<15}\")\n",
        "\n",
        "# Iterate through the evaluation results and print each row\n",
        "for result in evaluation_results:\n",
        "    row = [result[\"Learning Rate\"], result[\"Number of Iterations\"], result[\"Precision\"], result[\"Recall\"], result[\"F1 Score\"]]\n",
        "    # Format and print each row\n",
        "    print(f\"{row[0]:<15}{row[1]:<20}{row[2]:<15.2f}{row[3]:<15.2f}{row[4]:<15.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9l77sKlxuuS",
        "outputId": "d823af26-3e88-45bd-8283-958819f41a90"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate  Iterations          Precision      Recall         F1 Score       \n",
            "0.01           100                 0.99           0.94           0.97           \n",
            "0.01           500                 0.99           0.98           0.98           \n",
            "0.01           1000                0.99           0.99           0.99           \n",
            "0.01           2000                0.99           1.00           0.99           \n",
            "0.01           5000                0.99           0.99           0.99           \n",
            "0.1            100                 0.99           0.99           0.99           \n",
            "0.1            500                 0.99           0.99           0.99           \n",
            "0.1            1000                0.99           0.99           0.99           \n",
            "0.1            2000                0.99           1.00           0.99           \n",
            "0.1            5000                0.99           0.98           0.98           \n",
            "0.2            100                 0.99           1.00           0.99           \n",
            "0.2            500                 0.99           0.99           0.99           \n",
            "0.2            1000                0.99           1.00           0.99           \n",
            "0.2            2000                0.99           0.98           0.98           \n",
            "0.2            5000                0.99           0.98           0.98           \n",
            "0.5            100                 0.99           0.99           0.99           \n",
            "0.5            500                 0.99           0.99           0.99           \n",
            "0.5            1000                0.99           0.98           0.98           \n",
            "0.5            2000                0.99           0.98           0.98           \n",
            "0.5            5000                0.99           0.97           0.98           \n",
            "1.0            100                 0.99           0.99           0.99           \n",
            "1.0            500                 0.99           0.98           0.98           \n",
            "1.0            1000                0.99           0.98           0.98           \n",
            "1.0            2000                0.99           0.97           0.98           \n",
            "1.0            5000                0.99           0.97           0.98           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Learning Rate Impact:** Different learning rates have been tested (0.01, 0.1, 0.2, 0.5, 1.0). It's clear that the choice of learning rate has a significant impact on the evaluation metrics. Smaller learning rates (e.g., 0.01) seem to lead to slower but more consistent convergence, while larger learning rates (e.g., 1.0) can lead to more rapid convergence but might not always result in the best performance.\n",
        "\n",
        "2. **Number of Iterations:** The number of iterations has also been varied (100, 500, 1000, 2000, 5000). It's evident that, in general, as the number of iterations increases, the evaluation metrics tend to improve. However, there's a diminishing return effect where performance improvement plateaus with a sufficient number of iterations.\n",
        "\n",
        "3. **Precision, Recall, F1 Score:** Precision, recall, and F1 score are common evaluation metrics for binary classification. These metrics measure different aspects of the model's performance. High precision indicates a low rate of false positives, high recall indicates a low rate of false negatives, and a high F1 score is a balance between precision and recall.\n",
        "\n",
        "4. **Model Stability:** The model seems to achieve high precision and F1 score, suggesting that it performs well in correctly classifying positive cases (e.g., correctly identifying events). However, recall is not always perfect, indicating that it might miss some positive cases.\n",
        "\n",
        "5. **Optimal Hyperparameters:** Based on the provided data, it's not clear which combination of learning rate and number of iterations is the \"best.\" The choice of optimal hyperparameters can depend on the specific dataset and problem. You may want to consider factors such as the trade-off between training time and performance when selecting hyperparameters for your final model.\n",
        "\n",
        "6. **Regularization:** It's important to note that this analysis does not account for regularization techniques, which can also impact model performance.\n",
        "\n",
        "To further fine-tune your model, you can consider a more systematic hyperparameter search, such as using grid search or random search, to explore a broader range of hyperparameter values. Additionally, consider other techniques like cross-validation to obtain a more robust evaluation of model performance.\n",
        "\n",
        "The choice of hyperparameters ultimately depends on the specific characteristics of your dataset and the trade-offs you are willing to make between precision, recall, and training time."
      ],
      "metadata": {
        "id": "JpsSvL5d1RrP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4RHgBzVbout"
      },
      "execution_count": 85,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}